{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras_preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '../../sensix_daninhas/dataset_100x100/train'\n",
    "test_data_path  = '../../sensix_daninhas/dataset_100x100/validation'\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "epochs = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images :  193\n",
      "Test Images  :  43\n"
     ]
    }
   ],
   "source": [
    "num_of_train_samples = len(glob.glob(train_data_path + \"/**/*\" + \".jpg\" , recursive=True))\n",
    "num_of_test_samples  = len(glob.glob(test_data_path + \"/**/*\" + \".jpg\"  , recursive=True))\n",
    "print(\"Train Images : \", num_of_train_samples)\n",
    "print(\"Test Images  : \", num_of_test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 images belonging to 2 classes.\n",
      "Found 43 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_data_path,\n",
    "                                                    target_size=(img_rows, img_cols),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(test_data_path,\n",
    "                                                        target_size=(img_rows, img_cols),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), input_shape=(img_rows, img_cols, 3), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(32, (3, 3), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, (3, 3), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6/6 [==============================] - 3s 405ms/step - loss: 0.7895 - accuracy: 0.4686 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.7025 - accuracy: 0.4983 - val_loss: 0.6981 - val_accuracy: 0.4062\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.7138 - accuracy: 0.4267 - val_loss: 0.7320 - val_accuracy: 0.4688\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 2s 275ms/step - loss: 0.7116 - accuracy: 0.4864 - val_loss: 0.6930 - val_accuracy: 0.5625\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 0.6933 - accuracy: 0.5313 - val_loss: 0.6934 - val_accuracy: 0.4062\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 2s 282ms/step - loss: 0.6929 - accuracy: 0.4647 - val_loss: 0.6932 - val_accuracy: 0.4688\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 0.6925 - accuracy: 0.6183 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.6931 - accuracy: 0.5233 - val_loss: 0.6886 - val_accuracy: 0.5312\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.7008 - accuracy: 0.5946 - val_loss: 0.6859 - val_accuracy: 0.6562\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 0.7092 - accuracy: 0.5539 - val_loss: 0.6867 - val_accuracy: 0.6250\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 2s 298ms/step - loss: 0.6921 - accuracy: 0.5099 - val_loss: 0.7021 - val_accuracy: 0.4688\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 2s 273ms/step - loss: 0.9388 - accuracy: 0.5411 - val_loss: 0.6735 - val_accuracy: 0.5625\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 0.7068 - accuracy: 0.4223 - val_loss: 0.6932 - val_accuracy: 0.4062\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 2s 292ms/step - loss: 0.6982 - accuracy: 0.4943 - val_loss: 0.6793 - val_accuracy: 0.6250\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 2s 276ms/step - loss: 0.6886 - accuracy: 0.5516 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 2s 275ms/step - loss: 0.6931 - accuracy: 0.4887 - val_loss: 0.6851 - val_accuracy: 0.5938\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 2s 304ms/step - loss: 0.7032 - accuracy: 0.4725 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 0.6869 - accuracy: 0.5694 - val_loss: 0.6899 - val_accuracy: 0.5000\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 2s 284ms/step - loss: 0.7093 - accuracy: 0.3453 - val_loss: 0.6856 - val_accuracy: 0.5312\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.6977 - accuracy: 0.6061 - val_loss: 0.6900 - val_accuracy: 0.4688\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 0.6867 - accuracy: 0.5898 - val_loss: 0.7071 - val_accuracy: 0.4688\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 2s 252ms/step - loss: 0.6946 - accuracy: 0.5377 - val_loss: 0.8631 - val_accuracy: 0.5000\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.8829 - accuracy: 0.4925 - val_loss: 0.7552 - val_accuracy: 0.4375\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 0.6779 - accuracy: 0.5211 - val_loss: 0.6727 - val_accuracy: 0.4375\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 2s 276ms/step - loss: 0.6751 - accuracy: 0.5694 - val_loss: 0.7113 - val_accuracy: 0.5000\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 0.6738 - accuracy: 0.5654 - val_loss: 0.6877 - val_accuracy: 0.5312\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 2s 331ms/step - loss: 0.6439 - accuracy: 0.6765 - val_loss: 0.6296 - val_accuracy: 0.7500\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 2s 259ms/step - loss: 0.6827 - accuracy: 0.6681 - val_loss: 0.5679 - val_accuracy: 0.7500\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 2s 256ms/step - loss: 0.6067 - accuracy: 0.6350 - val_loss: 0.6311 - val_accuracy: 0.6250\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 2s 270ms/step - loss: 0.6152 - accuracy: 0.6252 - val_loss: 0.5588 - val_accuracy: 0.7812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2027eaafa0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "                    steps_per_epoch=num_of_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=num_of_test_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43877596 0.585687  ]\n",
      " [0.58458364 0.45956042]\n",
      " [0.57533765 0.46491796]\n",
      " [0.5705234  0.46739328]\n",
      " [0.5831297  0.45755923]\n",
      " [0.5895208  0.4583652 ]\n",
      " [0.5679772  0.4775984 ]\n",
      " [0.46353495 0.5619154 ]\n",
      " [0.57870966 0.46580034]\n",
      " [0.6195548  0.43487877]\n",
      " [0.44592625 0.5792037 ]\n",
      " [0.44406986 0.58032495]\n",
      " [0.58069503 0.4638746 ]\n",
      " [0.57654816 0.4641935 ]\n",
      " [0.5831101  0.45691705]\n",
      " [0.583577   0.46110144]\n",
      " [0.6015259  0.4454209 ]\n",
      " [0.58899087 0.45591962]\n",
      " [0.56278044 0.4823737 ]\n",
      " [0.4499036  0.5750998 ]\n",
      " [0.5794102  0.4651303 ]\n",
      " [0.57265663 0.46420068]\n",
      " [0.42771357 0.59706134]\n",
      " [0.57970214 0.46444845]\n",
      " [0.38796198 0.63907635]\n",
      " [0.6020934  0.4454913 ]\n",
      " [0.61281997 0.43741468]\n",
      " [0.4426346  0.58477795]\n",
      " [0.39609796 0.63123685]\n",
      " [0.46239778 0.5614244 ]\n",
      " [0.5828143  0.45963442]\n",
      " [0.61757123 0.44184324]\n",
      " [0.58804196 0.45772952]\n",
      " [0.5865634  0.4603808 ]\n",
      " [0.58221495 0.46139613]\n",
      " [0.5897338  0.45425758]\n",
      " [0.58696866 0.4546875 ]\n",
      " [0.38624138 0.64111304]\n",
      " [0.38832113 0.6383776 ]\n",
      " [0.579881   0.45831564]\n",
      " [0.5848859  0.45789176]\n",
      " [0.5829778  0.45886418]\n",
      " [0.6017529  0.44749442]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(validation_generator, num_of_test_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real :        [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1]\n",
      "Prediction :  [1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0]\n",
      "Confusion Matrix\n",
      "[[16  5]\n",
      " [15  7]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Real :       \", validation_generator.classes)\n",
    "print(\"Prediction : \", y_pred)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    daninhas       0.52      0.76      0.62        21\n",
      "sem_daninhas       0.58      0.32      0.41        22\n",
      "\n",
      "    accuracy                           0.53        43\n",
      "   macro avg       0.55      0.54      0.51        43\n",
      "weighted avg       0.55      0.53      0.51        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "target_names = ['daninhas', 'sem_daninhas']\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"daninhas_confusion_matrix_jp.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
